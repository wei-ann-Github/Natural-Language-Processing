{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\n\nimport pandas as pd\n# import pytorch_lightning as pl\nimport spacy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchtext\nfrom tqdm.notebook import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","execution_count":18,"outputs":[{"output_type":"stream","text":"cpu\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the data\ntrain_filename = '/kaggle/input/nlp-getting-started/train.csv'\ntr_df = pd.read_csv(train_filename, engine='python')\ndisplay(tr_df.head())\nprint(tr_df.shape)\nprint(tr_df.target.value_counts())","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","text":"(7613, 5)\n0    4342\n1    3271\nName: target, dtype: int64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Torchtext"},{"metadata":{"trusted":true},"cell_type":"code","source":"class spacy_tokenizer:\n    def __init__(self, lang):\n        self.lang = lang\n        self.nlp = spacy.blank(self.lang)\n        \n    def __call__(self, text):\n        return [token.text for token in self.nlp(text)]\n\ntokenizer = spacy_tokenizer('en')\nTEXT = torchtext.data.Field(tokenize=tokenizer, lower=True)\nLABEL = torchtext.data.LabelField(use_vocab=False)\n\nfields = [('id', None), ('keyword', None), ('location', None), ('text', TEXT), ('target', LABEL)]\ndataset = torchtext.data.TabularDataset(path=train_filename, format='csv', fields=fields, skip_header=True)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and validation dataset.\ntr_data, val_data = dataset.split(split_ratio=0.8)\nprint(len(tr_data), len(val_data))","execution_count":35,"outputs":[{"output_type":"stream","text":"6090 1523\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build vocab\nTEXT.build_vocab(tr_data)\nprint('num vocab', len(TEXT.vocab))\n\nmax_size = 20000\nTEXT.build_vocab(tr_data, max_size=max_size)\nprint('num vocab', len(TEXT.vocab))","execution_count":36,"outputs":[{"output_type":"stream","text":"num vocab 19417\nnum vocab 19417\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterator\nbatch_size = 32\ntr_iter = torchtext.data.BucketIterator(tr_data, batch_size=batch_size, shuffle=True, device=device)\nval_iter = torchtext.data.BucketIterator(val_data, batch_size=batch_size, shuffle=False, device=device)  # do not use train=False","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The network"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Rnn(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, bidirectional=False, n_layers=1, dropout=0, rnn_type='rnn'):\n        super().__init__()\n        self.bidirectional = bidirectional\n        self.n_layers = n_layers\n        self.rnn_type = rnn_type\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.encoder = getattr(nn, rnn_type.upper())(embedding_dim, hidden_dim, bidirectional=bidirectional, num_layers=n_layers, dropout=dropout)\n        \n        in_features = hidden_dim\n        if self.bidirectional:\n            in_features *= 2\n            \n        self.fc = nn.Linear(in_features=in_features, out_features=output_size)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        if self.rnn_type.upper() == 'LSTM':\n            x, (h_, c_) = self.encoder(x)\n        else:\n            x, h_ = self.encoder(x)\n        x = self.fc(h_)\n        \n        return x","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = len(TEXT.vocab)\nembedding_dim = 100\nhidden_dim = 100\noutput_size = 1\nbidirectional=False\nn_layers=1\ndropout=0\nrnn_type='rnn'\nlr = 0.0001\nnum_epochs = 20\n\nmodel = Rnn(input_size, embedding_dim, hidden_dim, output_size, bidirectional=bidirectional, n_layers=n_layers, dropout=dropout, rnn_type=rnn_type).to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.BCEWithLogitsLoss()\n\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    tr_loss = 0\n    for idx, batch in enumerate(tr_iter):\n        optimizer.zero_grad()\n        \n        x, y = batch\n        \n        logits = model(x)\n        loss = criterion(logits.squeeze().to(float), y.to(float))\n        tr_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        \n    tr_loss /= (idx + 1)\n    \n    with torch.no_grad():\n        model.eval()\n        num_correct = 0\n        val_loss = 0\n        for idx, batch in enumerate(val_iter):\n            x, y = batch\n            \n            logits = model(x)\n            loss = criterion(logits.squeeze().to(float), y.to(float))\n            val_loss = loss.item()\n            \n            \n            prediction = torch.sigmoid(logits)\n            prediction = (prediction >= 0.5).to(int).squeeze()\n            num_correct += sum(prediction == y)\n            \n        val_loss = loss / (idx + 1)\n        accuracy = num_correct.item() / len(val_data) * 100\n    \n    print(f'End epoch {epoch + 1}: Train loss {tr_loss:.6f}, Validation loss {val_loss:.6f}, Accuracy {accuracy:.4f} %')","execution_count":80,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f17e98ab28c42c9b4c16097d9136578"}},"metadata":{}},{"output_type":"stream","text":"End epoch 1: Train loss 0.686492, Validation loss 0.014709, Accuracy 56.9928 %\nEnd epoch 2: Train loss 0.685640, Validation loss 0.014543, Accuracy 57.3211 %\nEnd epoch 3: Train loss 0.684400, Validation loss 0.014557, Accuracy 57.7150 %\n\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}